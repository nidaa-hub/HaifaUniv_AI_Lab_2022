{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nidaa-hub/HaifaUniv_AI_Lab_2022/blob/main/AI_2022_Lab_5_B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOcR829yaN5A"
      },
      "source": [
        "Press ▶ or ctrl+Enter\n",
        "* ID Submetter1: 316151232\n",
        "* ID Submetter2: 038163630"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "התקנות וספריות"
      ],
      "metadata": {
        "id": "jLhX1UasOCWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#///////////////////////lab 5.2///////////////////////\n",
        "# importing lybraries and defininig functions\n",
        "import pandas as pd\n",
        "import pandas.testing as tm\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "\n",
        "CRED = '\\033[91m'\n",
        "CGREEN = '\\033[92m'\n",
        "CFLASH = '\\033[5m'\n",
        "CBOLD = '\\33[1m'\n",
        "CITALIC = '\\33[3m'\n",
        "CEND = '\\033[0m'"
      ],
      "metadata": {
        "id": "RvmijJrTOBIi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "הורידו וקיראו את הדאטה\n",
        "\n",
        "[link text](https://archive.ics.uci.edu/ml/datasets/glass+identification)\n"
      ],
      "metadata": {
        "id": "SodME5hWM-AU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_url = (\"https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data\")\n",
        "df = pd.read_csv(target_url, header=None)\n",
        "df.columns = ['', '', '', '', '', '','', '', '', '', 'GlassType']\n",
        "print('Data File:')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LbuMx6-3G-6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "4da46de4-6ed0-4a3a-fb75-f5cecc137037"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data File:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                               GlassType\n",
              "0  1  1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.0  0.0          1\n",
              "1  2  1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.0  0.0          1\n",
              "2  3  1.51618  13.53  3.55  1.54  72.99  0.39  7.78  0.0  0.0          1\n",
              "3  4  1.51766  13.21  3.69  1.29  72.61  0.57  8.22  0.0  0.0          1\n",
              "4  5  1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.0  0.0          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ea8a5f6f-871c-4ca9-a264-6f6a1bc4986d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>GlassType</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.52101</td>\n",
              "      <td>13.64</td>\n",
              "      <td>4.49</td>\n",
              "      <td>1.10</td>\n",
              "      <td>71.78</td>\n",
              "      <td>0.06</td>\n",
              "      <td>8.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1.51761</td>\n",
              "      <td>13.89</td>\n",
              "      <td>3.60</td>\n",
              "      <td>1.36</td>\n",
              "      <td>72.73</td>\n",
              "      <td>0.48</td>\n",
              "      <td>7.83</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.51618</td>\n",
              "      <td>13.53</td>\n",
              "      <td>3.55</td>\n",
              "      <td>1.54</td>\n",
              "      <td>72.99</td>\n",
              "      <td>0.39</td>\n",
              "      <td>7.78</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1.51766</td>\n",
              "      <td>13.21</td>\n",
              "      <td>3.69</td>\n",
              "      <td>1.29</td>\n",
              "      <td>72.61</td>\n",
              "      <td>0.57</td>\n",
              "      <td>8.22</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1.51742</td>\n",
              "      <td>13.27</td>\n",
              "      <td>3.62</td>\n",
              "      <td>1.24</td>\n",
              "      <td>73.08</td>\n",
              "      <td>0.55</td>\n",
              "      <td>8.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ea8a5f6f-871c-4ca9-a264-6f6a1bc4986d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ea8a5f6f-871c-4ca9-a264-6f6a1bc4986d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ea8a5f6f-871c-4ca9-a264-6f6a1bc4986d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "התאימו קוד בין 0-ל5 לכל סוג זכוכית אפשרי - ציינו את\n",
        "ההתפלגות של הדוגמאות לפי כל סוג זכוכית."
      ],
      "metadata": {
        "id": "JPH7qoZRO2HC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use('dark_background')\n",
        "plt.figure(figsize=(10,5))\n",
        "figure, ax = plt.subplots(1,1, figsize=(10,5))\n",
        "sns.countplot(x = 'GlassType', data=df)\n",
        "ax.set_xticklabels( ('0','1','2','3','4','5') )\n",
        "plt.show()\n",
        "df['GlassType'].value_counts()"
      ],
      "metadata": {
        "id": "0FpcwhYvO27b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "35d659d6-5e2e-4720-e1e4-f484d32eded0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    76\n",
              "1    70\n",
              "7    29\n",
              "3    17\n",
              "5    13\n",
              "6     9\n",
              "Name: GlassType, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "חלקו את הדאטה לקבוצת אימון %80 וקבוצת בדיקה %20 "
      ],
      "metadata": {
        "id": "ened-PZrRKS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = df.values[:, -1]\n",
        "x_train, x_test, y_train, y_test = train_test_split(df, label, stratify=label, test_size=0.2, random_state=1)\n",
        "print(\"Train part:\")\n",
        "print(\"Train X:\")\n",
        "print(x_train)\n",
        "print(\"Train Y:\")\n",
        "print(y_train)\n",
        "print(\"------------------------------\")\n",
        "print(\"Test part:\")\n",
        "print(\"Test X:\")\n",
        "print(x_test)\n",
        "print(\"Test Y:\")\n",
        "print(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IJ9EQTTSJ_D",
        "outputId": "8f82f822-d784-4f02-9cf7-d8e405b6f093"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train part:\n",
            "Train X:\n",
            "                                                                      \\\n",
            "96    97  1.51841  13.02  3.62  1.06  72.34  0.64   9.13  0.00  0.15   \n",
            "98    99  1.51689  12.67  2.88  1.71  73.21  0.73   8.54  0.00  0.00   \n",
            "194  195  1.51683  14.56  0.00  1.98  73.29  0.00   8.52  1.57  0.07   \n",
            "106  107  1.53125  10.73  0.00  2.10  69.81  0.58  13.30  3.15  0.28   \n",
            "209  210  1.51623  14.14  0.00  2.88  72.61  0.08   9.18  1.06  0.00   \n",
            "..   ...      ...    ...   ...   ...    ...   ...    ...   ...   ...   \n",
            "99   100  1.51811  12.96  2.96  1.43  72.92  0.60   8.79  0.14  0.00   \n",
            "124  125  1.52177  13.20  3.68  1.15  72.75  0.54   8.52  0.00  0.00   \n",
            "208  209  1.51640  14.37  0.00  2.74  72.85  0.00   9.45  0.54  0.00   \n",
            "153  154  1.51610  13.42  3.40  1.22  72.69  0.59   8.32  0.00  0.00   \n",
            "23    24  1.51751  12.81  3.57  1.35  73.02  0.62   8.59  0.00  0.00   \n",
            "\n",
            "     GlassType  \n",
            "96           2  \n",
            "98           2  \n",
            "194          7  \n",
            "106          2  \n",
            "209          7  \n",
            "..         ...  \n",
            "99           2  \n",
            "124          2  \n",
            "208          7  \n",
            "153          3  \n",
            "23           1  \n",
            "\n",
            "[171 rows x 11 columns]\n",
            "Train Y:\n",
            "[2. 2. 7. 2. 7. 2. 2. 1. 2. 2. 1. 2. 3. 1. 7. 7. 7. 7. 6. 1. 2. 1. 5. 1.\n",
            " 1. 2. 2. 7. 2. 1. 3. 2. 1. 2. 1. 3. 2. 2. 6. 1. 3. 3. 1. 1. 2. 7. 2. 2.\n",
            " 2. 2. 2. 7. 5. 1. 5. 1. 2. 7. 2. 1. 2. 3. 1. 3. 1. 2. 1. 3. 2. 1. 2. 1.\n",
            " 2. 7. 2. 5. 3. 2. 2. 7. 2. 2. 1. 7. 1. 3. 1. 1. 2. 1. 1. 1. 5. 1. 5. 1.\n",
            " 7. 5. 2. 6. 7. 1. 2. 2. 3. 1. 1. 1. 7. 2. 7. 1. 3. 2. 2. 6. 2. 1. 1. 2.\n",
            " 2. 2. 7. 1. 5. 2. 6. 7. 6. 2. 6. 7. 2. 1. 2. 1. 5. 2. 1. 1. 1. 1. 2. 1.\n",
            " 2. 2. 2. 1. 2. 1. 7. 2. 2. 5. 2. 1. 1. 1. 7. 1. 1. 1. 2. 3. 1. 1. 2. 2.\n",
            " 7. 3. 1.]\n",
            "------------------------------\n",
            "Test part:\n",
            "Test X:\n",
            "                                                                      \\\n",
            "78    79  1.51613  13.92  3.52  1.25  72.88  0.37   7.94  0.00  0.14   \n",
            "104  105  1.52410  13.83  2.90  1.17  71.15  0.08  10.79  0.00  0.00   \n",
            "135  136  1.51789  13.19  3.90  1.30  72.33  0.55   8.44  0.00  0.28   \n",
            "85    86  1.51625  13.36  3.58  1.49  72.72  0.45   8.21  0.00  0.00   \n",
            "191  192  1.51602  14.85  0.00  2.38  73.28  0.00   8.76  0.64  0.09   \n",
            "50    51  1.52320  13.72  3.72  0.51  71.75  0.09  10.06  0.00  0.16   \n",
            "80    81  1.51592  12.86  3.52  2.12  72.66  0.69   7.97  0.00  0.00   \n",
            "168  169  1.51666  12.86  0.00  1.83  73.88  0.97  10.17  0.00  0.00   \n",
            "12    13  1.51589  12.88  3.43  1.40  73.28  0.69   8.05  0.00  0.24   \n",
            "6      7  1.51743  13.30  3.60  1.14  73.09  0.58   8.17  0.00  0.00   \n",
            "155  156  1.51646  13.04  3.40  1.26  73.01  0.52   8.58  0.00  0.00   \n",
            "45    46  1.51900  13.49  3.48  1.35  71.95  0.55   9.00  0.00  0.00   \n",
            "20    21  1.51750  12.82  3.55  1.49  72.75  0.54   8.52  0.00  0.19   \n",
            "97    98  1.51743  12.20  3.25  1.16  73.55  0.62   8.90  0.00  0.24   \n",
            "68    69  1.52152  13.12  3.58  0.90  72.20  0.23   9.82  0.00  0.16   \n",
            "164  165  1.51915  12.73  1.85  1.86  72.69  0.60  10.09  0.00  0.00   \n",
            "127  128  1.52081  13.78  2.28  1.43  71.99  0.49   9.85  0.00  0.17   \n",
            "108  109  1.52222  14.43  0.00  1.00  72.67  0.10  11.52  0.00  0.08   \n",
            "169  170  1.51994  13.27  0.00  1.76  73.03  0.47  11.32  0.00  0.00   \n",
            "64    65  1.52172  13.48  3.74  0.90  72.01  0.18   9.61  0.00  0.07   \n",
            "205  206  1.51732  14.95  0.00  1.80  72.99  0.00   8.61  1.55  0.00   \n",
            "148  149  1.51670  13.24  3.57  1.38  72.70  0.56   8.44  0.00  0.10   \n",
            "198  199  1.51531  14.38  0.00  2.66  73.10  0.04   9.08  0.64  0.00   \n",
            "213  214  1.51711  14.23  0.00  2.08  73.36  0.00   8.62  1.67  0.00   \n",
            "5      6  1.51596  12.79  3.61  1.62  72.97  0.64   8.07  0.00  0.26   \n",
            "9     10  1.51755  13.00  3.60  1.36  72.99  0.57   8.40  0.00  0.11   \n",
            "95    96  1.51860  13.36  3.43  1.43  72.26  0.51   8.60  0.00  0.00   \n",
            "212  213  1.51651  14.38  0.00  1.94  73.61  0.00   8.48  1.57  0.00   \n",
            "183  184  1.51969  14.56  0.00  0.56  73.48  0.00  11.22  0.00  0.00   \n",
            "7      8  1.51756  13.15  3.61  1.05  73.24  0.57   8.24  0.00  0.00   \n",
            "196  197  1.51556  13.87  0.00  2.54  73.23  0.14   9.41  0.81  0.01   \n",
            "1      2  1.51761  13.89  3.60  1.36  72.73  0.48   7.83  0.00  0.00   \n",
            "107  108  1.53393  12.30  0.00  1.00  70.16  0.12  16.19  0.00  0.24   \n",
            "144  145  1.51660  12.99  3.18  1.23  72.97  0.58   8.81  0.00  0.24   \n",
            "37    38  1.51797  12.74  3.48  1.35  72.96  0.64   8.68  0.00  0.00   \n",
            "25    26  1.51764  12.98  3.54  1.21  73.00  0.65   8.53  0.00  0.00   \n",
            "137  138  1.51711  12.89  3.62  1.57  72.96  0.61   8.11  0.00  0.00   \n",
            "16    17  1.51784  12.68  3.67  1.16  73.11  0.61   8.70  0.00  0.00   \n",
            "93    94  1.51590  13.24  3.34  1.47  73.10  0.39   8.22  0.00  0.00   \n",
            "117  118  1.51708  13.72  3.68  1.81  72.06  0.64   7.88  0.00  0.00   \n",
            "181  182  1.51888  14.99  0.78  1.74  72.50  0.00   9.95  0.00  0.00   \n",
            "152  153  1.51779  13.64  3.65  0.65  73.00  0.06   8.93  0.00  0.00   \n",
            "102  103  1.51820  12.62  2.76  0.83  73.81  0.35   9.42  0.00  0.20   \n",
            "\n",
            "     GlassType  \n",
            "78           2  \n",
            "104          2  \n",
            "135          2  \n",
            "85           2  \n",
            "191          7  \n",
            "50           1  \n",
            "80           2  \n",
            "168          5  \n",
            "12           1  \n",
            "6            1  \n",
            "155          3  \n",
            "45           1  \n",
            "20           1  \n",
            "97           2  \n",
            "68           1  \n",
            "164          5  \n",
            "127          2  \n",
            "108          2  \n",
            "169          5  \n",
            "64           1  \n",
            "205          7  \n",
            "148          3  \n",
            "198          7  \n",
            "213          7  \n",
            "5            1  \n",
            "9            1  \n",
            "95           2  \n",
            "212          7  \n",
            "183          6  \n",
            "7            1  \n",
            "196          7  \n",
            "1            1  \n",
            "107          2  \n",
            "144          2  \n",
            "37           1  \n",
            "25           1  \n",
            "137          2  \n",
            "16           1  \n",
            "93           2  \n",
            "117          2  \n",
            "181          6  \n",
            "152          3  \n",
            "102          2  \n",
            "Test Y:\n",
            "[2. 2. 2. 2. 7. 1. 2. 5. 1. 1. 3. 1. 1. 2. 1. 5. 2. 2. 5. 1. 7. 3. 7. 7.\n",
            " 1. 1. 2. 7. 6. 1. 7. 1. 2. 2. 1. 1. 2. 1. 2. 2. 6. 3. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "נרמלו את מאפייני הדאטה להתפלגות נורמלית סטנדרטית (ממוצע\n",
        "0 וסטית תקן 1) שיהווה קלט לרשת"
      ],
      "metadata": {
        "id": "HYJafQB9TYJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Data before Normalizing : \")\n",
        "print(df)\n",
        "normalized = pd.DataFrame((preprocessing.MinMaxScaler()).fit_transform(df))\n",
        "print(\"Data after Normalizing : \")\n",
        "print(normalized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCRpyO2kRYIU",
        "outputId": "dbb9fc75-49c6-4cfd-91d1-3db16014c3bb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data before Normalizing : \n",
            "                                                                    GlassType\n",
            "0      1  1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.00  0.0          1\n",
            "1      2  1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.00  0.0          1\n",
            "2      3  1.51618  13.53  3.55  1.54  72.99  0.39  7.78  0.00  0.0          1\n",
            "3      4  1.51766  13.21  3.69  1.29  72.61  0.57  8.22  0.00  0.0          1\n",
            "4      5  1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.00  0.0          1\n",
            "..   ...      ...    ...   ...   ...    ...   ...   ...   ...  ...        ...\n",
            "209  210  1.51623  14.14  0.00  2.88  72.61  0.08  9.18  1.06  0.0          7\n",
            "210  211  1.51685  14.92  0.00  1.99  73.06  0.00  8.40  1.59  0.0          7\n",
            "211  212  1.52065  14.36  0.00  2.02  73.42  0.00  8.44  1.64  0.0          7\n",
            "212  213  1.51651  14.38  0.00  1.94  73.61  0.00  8.48  1.57  0.0          7\n",
            "213  214  1.51711  14.23  0.00  2.08  73.36  0.00  8.62  1.67  0.0          7\n",
            "\n",
            "[214 rows x 11 columns]\n",
            "Data after Normalizing : \n",
            "           0         1         2         3         4         5         6   \\\n",
            "0    0.000000  0.432836  0.437594  1.000000  0.252336  0.351786  0.009662   \n",
            "1    0.004695  0.283582  0.475188  0.801782  0.333333  0.521429  0.077295   \n",
            "2    0.009390  0.220808  0.421053  0.790646  0.389408  0.567857  0.062802   \n",
            "3    0.014085  0.285777  0.372932  0.821826  0.311526  0.500000  0.091787   \n",
            "4    0.018779  0.275241  0.381955  0.806236  0.295950  0.583929  0.088567   \n",
            "..        ...       ...       ...       ...       ...       ...       ...   \n",
            "209  0.981221  0.223003  0.512782  0.000000  0.806854  0.500000  0.012882   \n",
            "210  0.985915  0.250219  0.630075  0.000000  0.529595  0.580357  0.000000   \n",
            "211  0.990610  0.417032  0.545865  0.000000  0.538941  0.644643  0.000000   \n",
            "212  0.995305  0.235294  0.548872  0.000000  0.514019  0.678571  0.000000   \n",
            "213  1.000000  0.261633  0.526316  0.000000  0.557632  0.633929  0.000000   \n",
            "\n",
            "           7         8    9    10  \n",
            "0    0.308550  0.000000  0.0  0.0  \n",
            "1    0.223048  0.000000  0.0  0.0  \n",
            "2    0.218401  0.000000  0.0  0.0  \n",
            "3    0.259294  0.000000  0.0  0.0  \n",
            "4    0.245353  0.000000  0.0  0.0  \n",
            "..        ...       ...  ...  ...  \n",
            "209  0.348513  0.336508  0.0  1.0  \n",
            "210  0.276022  0.504762  0.0  1.0  \n",
            "211  0.279740  0.520635  0.0  1.0  \n",
            "212  0.283457  0.498413  0.0  1.0  \n",
            "213  0.296468  0.530159  0.0  1.0  \n",
            "\n",
            "[214 rows x 11 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#don't run this\n",
        "label = normalized.values[:, -1]\n",
        "x_train, x_test, y_train, y_test = train_test_split(normalized, label, stratify=label, test_size=0.2, random_state=1)\n",
        "print(\"Train part:\")\n",
        "print(\"X:\")\n",
        "print(x_train)\n",
        "print(\"Y:\")\n",
        "print(y_train)\n",
        "print(\"------------------------------\")\n",
        "print(\"Test part:\")\n",
        "print(\"X:\")\n",
        "print(x_test)\n",
        "print(\"Y:\")\n",
        "print(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnLT9V1S4RCs",
        "outputId": "06b7a641-852c-4310-ca8e-2d827a84cc8f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train part:\n",
            "X:\n",
            "           0         1         2         3         4         5         6   \\\n",
            "96   0.450704  0.318701  0.344361  0.806236  0.239875  0.451786  0.103060   \n",
            "98   0.460094  0.251975  0.291729  0.641425  0.442368  0.607143  0.117552   \n",
            "194  0.910798  0.249342  0.575940  0.000000  0.526480  0.621429  0.000000   \n",
            "106  0.497653  0.882353  0.000000  0.000000  0.563863  0.000000  0.093398   \n",
            "209  0.981221  0.223003  0.512782  0.000000  0.806854  0.500000  0.012882   \n",
            "..        ...       ...       ...       ...       ...       ...       ...   \n",
            "99   0.464789  0.305531  0.335338  0.659243  0.355140  0.555357  0.096618   \n",
            "124  0.582160  0.466198  0.371429  0.819599  0.267913  0.525000  0.086957   \n",
            "208  0.976526  0.230465  0.547368  0.000000  0.763240  0.542857  0.000000   \n",
            "153  0.718310  0.217296  0.404511  0.757238  0.289720  0.514286  0.095008   \n",
            "23   0.107981  0.279192  0.312782  0.795100  0.330218  0.573214  0.099839   \n",
            "\n",
            "           7         8         9         10  \n",
            "96   0.343866  0.000000  0.294118  0.166667  \n",
            "98   0.289033  0.000000  0.000000  0.166667  \n",
            "194  0.287175  0.498413  0.137255  1.000000  \n",
            "106  0.731413  1.000000  0.549020  0.166667  \n",
            "209  0.348513  0.336508  0.000000  1.000000  \n",
            "..        ...       ...       ...       ...  \n",
            "99   0.312268  0.044444  0.000000  0.166667  \n",
            "124  0.287175  0.000000  0.000000  0.166667  \n",
            "208  0.373606  0.171429  0.000000  1.000000  \n",
            "153  0.268587  0.000000  0.000000  0.333333  \n",
            "23   0.293680  0.000000  0.000000  0.000000  \n",
            "\n",
            "[171 rows x 11 columns]\n",
            "Y:\n",
            "[0.16666667 0.16666667 1.         0.16666667 1.         0.16666667\n",
            " 0.16666667 0.         0.16666667 0.16666667 0.         0.16666667\n",
            " 0.33333333 0.         1.         1.         1.         1.\n",
            " 0.83333333 0.         0.16666667 0.         0.66666667 0.\n",
            " 0.         0.16666667 0.16666667 1.         0.16666667 0.\n",
            " 0.33333333 0.16666667 0.         0.16666667 0.         0.33333333\n",
            " 0.16666667 0.16666667 0.83333333 0.         0.33333333 0.33333333\n",
            " 0.         0.         0.16666667 1.         0.16666667 0.16666667\n",
            " 0.16666667 0.16666667 0.16666667 1.         0.66666667 0.\n",
            " 0.66666667 0.         0.16666667 1.         0.16666667 0.\n",
            " 0.16666667 0.33333333 0.         0.33333333 0.         0.16666667\n",
            " 0.         0.33333333 0.16666667 0.         0.16666667 0.\n",
            " 0.16666667 1.         0.16666667 0.66666667 0.33333333 0.16666667\n",
            " 0.16666667 1.         0.16666667 0.16666667 0.         1.\n",
            " 0.         0.33333333 0.         0.         0.16666667 0.\n",
            " 0.         0.         0.66666667 0.         0.66666667 0.\n",
            " 1.         0.66666667 0.16666667 0.83333333 1.         0.\n",
            " 0.16666667 0.16666667 0.33333333 0.         0.         0.\n",
            " 1.         0.16666667 1.         0.         0.33333333 0.16666667\n",
            " 0.16666667 0.83333333 0.16666667 0.         0.         0.16666667\n",
            " 0.16666667 0.16666667 1.         0.         0.66666667 0.16666667\n",
            " 0.83333333 1.         0.83333333 0.16666667 0.83333333 1.\n",
            " 0.16666667 0.         0.16666667 0.         0.66666667 0.16666667\n",
            " 0.         0.         0.         0.         0.16666667 0.\n",
            " 0.16666667 0.16666667 0.16666667 0.         0.16666667 0.\n",
            " 1.         0.16666667 0.16666667 0.66666667 0.16666667 0.\n",
            " 0.         0.         1.         0.         0.         0.\n",
            " 0.16666667 0.33333333 0.         0.         0.16666667 0.16666667\n",
            " 1.         0.33333333 0.        ]\n",
            "------------------------------\n",
            "Test part:\n",
            "X:\n",
            "           0         1         2         3         4         5         6   \\\n",
            "78   0.366197  0.218613  0.479699  0.783964  0.299065  0.548214  0.059581   \n",
            "104  0.488263  0.568481  0.466165  0.645880  0.274143  0.239286  0.012882   \n",
            "135  0.633803  0.295874  0.369925  0.868597  0.314642  0.450000  0.088567   \n",
            "85   0.399061  0.223881  0.395489  0.797327  0.373832  0.519643  0.072464   \n",
            "191  0.896714  0.213784  0.619549  0.000000  0.651090  0.619643  0.000000   \n",
            "50   0.234742  0.528973  0.449624  0.828508  0.068536  0.346429  0.014493   \n",
            "80   0.375587  0.209394  0.320301  0.783964  0.570093  0.508929  0.111111   \n",
            "168  0.788732  0.241879  0.320301  0.000000  0.479751  0.726786  0.156200   \n",
            "12   0.056338  0.208077  0.323308  0.763920  0.345794  0.619643  0.111111   \n",
            "6    0.028169  0.275680  0.386466  0.801782  0.264798  0.585714  0.093398   \n",
            "155  0.727700  0.233099  0.347368  0.757238  0.302181  0.571429  0.083736   \n",
            "45   0.211268  0.344601  0.415038  0.775056  0.330218  0.382143  0.088567   \n",
            "20   0.093897  0.278753  0.314286  0.790646  0.373832  0.525000  0.086957   \n",
            "97   0.455399  0.275680  0.221053  0.723831  0.271028  0.667857  0.099839   \n",
            "68   0.319249  0.455224  0.359398  0.797327  0.190031  0.426786  0.037037   \n",
            "164  0.769953  0.351185  0.300752  0.412027  0.489097  0.514286  0.096618   \n",
            "127  0.596244  0.424056  0.458647  0.507795  0.355140  0.389286  0.078905   \n",
            "108  0.507042  0.485953  0.556391  0.000000  0.221184  0.510714  0.016103   \n",
            "169  0.793427  0.385865  0.381955  0.000000  0.457944  0.575000  0.075684   \n",
            "64   0.300469  0.464004  0.413534  0.832962  0.190031  0.392857  0.028986   \n",
            "205  0.962441  0.270852  0.634586  0.000000  0.470405  0.567857  0.000000   \n",
            "148  0.694836  0.243635  0.377444  0.795100  0.339564  0.516071  0.090177   \n",
            "198  0.929577  0.182616  0.548872  0.000000  0.738318  0.587500  0.006441   \n",
            "213  1.000000  0.261633  0.526316  0.000000  0.557632  0.633929  0.000000   \n",
            "5    0.023474  0.211150  0.309774  0.804009  0.414330  0.564286  0.103060   \n",
            "9    0.042254  0.280948  0.341353  0.801782  0.333333  0.567857  0.091787   \n",
            "95   0.446009  0.327041  0.395489  0.763920  0.355140  0.437500  0.082126   \n",
            "212  0.995305  0.235294  0.548872  0.000000  0.514019  0.678571  0.000000   \n",
            "183  0.859155  0.374890  0.575940  0.000000  0.084112  0.655357  0.000000   \n",
            "7    0.032864  0.281387  0.363910  0.804009  0.236760  0.612500  0.091787   \n",
            "196  0.920188  0.193591  0.472180  0.000000  0.700935  0.610714  0.022544   \n",
            "1    0.004695  0.283582  0.475188  0.801782  0.333333  0.521429  0.077295   \n",
            "107  0.502347  1.000000  0.236090  0.000000  0.221184  0.062500  0.019324   \n",
            "144  0.676056  0.239245  0.339850  0.708241  0.292835  0.564286  0.093398   \n",
            "37   0.173709  0.299385  0.302256  0.775056  0.330218  0.562500  0.103060   \n",
            "25   0.117371  0.284899  0.338346  0.788419  0.286604  0.569643  0.104670   \n",
            "137  0.643192  0.261633  0.324812  0.806236  0.398754  0.562500  0.098229   \n",
            "16   0.075117  0.293679  0.293233  0.817372  0.271028  0.589286  0.098229   \n",
            "93   0.436620  0.208516  0.377444  0.743875  0.367601  0.587500  0.062802   \n",
            "117  0.549296  0.260316  0.449624  0.819599  0.473520  0.401786  0.103060   \n",
            "181  0.849765  0.339333  0.640602  0.173719  0.451713  0.480357  0.000000   \n",
            "152  0.713615  0.291484  0.437594  0.812918  0.112150  0.569643  0.009662   \n",
            "102  0.478873  0.309482  0.284211  0.614699  0.168224  0.714286  0.056361   \n",
            "\n",
            "           7         8         9         10  \n",
            "78   0.233271  0.000000  0.274510  0.166667  \n",
            "104  0.498141  0.000000  0.000000  0.166667  \n",
            "135  0.279740  0.000000  0.549020  0.166667  \n",
            "85   0.258364  0.000000  0.000000  0.166667  \n",
            "191  0.309480  0.203175  0.176471  1.000000  \n",
            "50   0.430297  0.000000  0.313725  0.000000  \n",
            "80   0.236059  0.000000  0.000000  0.166667  \n",
            "168  0.440520  0.000000  0.000000  0.666667  \n",
            "12   0.243494  0.000000  0.470588  0.000000  \n",
            "6    0.254647  0.000000  0.000000  0.000000  \n",
            "155  0.292751  0.000000  0.000000  0.333333  \n",
            "45   0.331784  0.000000  0.000000  0.000000  \n",
            "20   0.287175  0.000000  0.372549  0.000000  \n",
            "97   0.322491  0.000000  0.470588  0.166667  \n",
            "68   0.407993  0.000000  0.313725  0.000000  \n",
            "164  0.433086  0.000000  0.000000  0.666667  \n",
            "127  0.410781  0.000000  0.333333  0.166667  \n",
            "108  0.565985  0.000000  0.156863  0.166667  \n",
            "169  0.547398  0.000000  0.000000  0.666667  \n",
            "64   0.388476  0.000000  0.137255  0.000000  \n",
            "205  0.295539  0.492063  0.000000  1.000000  \n",
            "148  0.279740  0.000000  0.196078  0.333333  \n",
            "198  0.339219  0.203175  0.000000  1.000000  \n",
            "213  0.296468  0.530159  0.000000  1.000000  \n",
            "5    0.245353  0.000000  0.509804  0.000000  \n",
            "9    0.276022  0.000000  0.215686  0.000000  \n",
            "95   0.294610  0.000000  0.000000  0.166667  \n",
            "212  0.283457  0.498413  0.000000  1.000000  \n",
            "183  0.538104  0.000000  0.000000  0.833333  \n",
            "7    0.261152  0.000000  0.000000  0.000000  \n",
            "196  0.369888  0.257143  0.019608  1.000000  \n",
            "1    0.223048  0.000000  0.000000  0.000000  \n",
            "107  1.000000  0.000000  0.470588  0.166667  \n",
            "144  0.314126  0.000000  0.470588  0.166667  \n",
            "37   0.302045  0.000000  0.000000  0.000000  \n",
            "25   0.288104  0.000000  0.000000  0.000000  \n",
            "137  0.249071  0.000000  0.000000  0.166667  \n",
            "16   0.303903  0.000000  0.000000  0.000000  \n",
            "93   0.259294  0.000000  0.000000  0.166667  \n",
            "117  0.227695  0.000000  0.000000  0.166667  \n",
            "181  0.420074  0.000000  0.000000  0.833333  \n",
            "152  0.325279  0.000000  0.000000  0.333333  \n",
            "102  0.370818  0.000000  0.392157  0.166667  \n",
            "Y:\n",
            "[0.16666667 0.16666667 0.16666667 0.16666667 1.         0.\n",
            " 0.16666667 0.66666667 0.         0.         0.33333333 0.\n",
            " 0.         0.16666667 0.         0.66666667 0.16666667 0.16666667\n",
            " 0.66666667 0.         1.         0.33333333 1.         1.\n",
            " 0.         0.         0.16666667 1.         0.83333333 0.\n",
            " 1.         0.         0.16666667 0.16666667 0.         0.\n",
            " 0.16666667 0.         0.16666667 0.16666667 0.83333333 0.33333333\n",
            " 0.16666667]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " FULLY\n",
        "CONNECTED בעלת קישוריות מלאה MLP יצרו רשת נוירונים פשוטה \n",
        "\n",
        "(logits)  דו-שכבתית המקבלת קלטים של 9 המאפיינים מסווגת אותם ומוציאה שישה פלטים "
      ],
      "metadata": {
        "id": "PXGxTLRDo43Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import neural_network\n",
        "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
        "import numpy as np\n",
        "mlp = neural_network.MLPClassifier(max_iter=50000,hidden_layer_sizes=(9,6))"
      ],
      "metadata": {
        "id": "mgqrBohpqFuG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " SOFTMAX את ששת הפלטים יש לאגד לפונקצית הסתברות באמצעות\n",
        " פונקצית"
      ],
      "metadata": {
        "id": "Bi5_liaxtyvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0) # only difference\n",
        "\n",
        "\n",
        "print(mlp)\n",
        "mlp.fit(x_train,y_train)\n",
        "print(mlp.predict_proba(x_test[:1]))\n",
        "print(softmax(mlp.predict_proba(x_test[:1])))\n",
        "print(mlp.predict(x_test))\n",
        "#print(y_test)\n",
        "#print(mlp.score(x_test, y_test))\n",
        "#Predict\n",
        "y_pred = mlp.predict(x_test)\n",
        "#y_pred = softmax(x_pred)\n",
        "\n",
        "#x_pred = mlp.predict_proba(x_test[:1])\n",
        "#y_pred = softmax(x_pred)\n",
        "\n",
        "#Score\n",
        "acu = accuracy_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "print(CGREEN + \"Confusion matrix:\\n\",cm)\n",
        "\n",
        "print(CBOLD + \"\\n Accuracy of ANN:\",acu)\n",
        "print(\"\\n\"+ CEND)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UM3ljjPatGJN",
        "outputId": "b163fbcd-ff34-4adc-94b9-8b0ec6eb288d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLPClassifier(hidden_layer_sizes=(9, 6), max_iter=50000)\n",
            "[[6.74204405e-02 9.29601016e-01 1.25804748e-05 2.96595305e-03\n",
            "  9.71312262e-09 3.53185420e-17]]\n",
            "[[1. 1. 1. 1. 1. 1.]]\n",
            "[2. 2. 2. 2. 7. 1. 2. 5. 1. 1. 3. 1. 1. 2. 1. 5. 2. 2. 5. 1. 7. 3. 7. 7.\n",
            " 1. 1. 2. 7. 6. 1. 7. 1. 2. 2. 1. 1. 2. 1. 2. 2. 6. 3. 2.]\n",
            "\u001b[92mConfusion matrix:\n",
            " [[14  0  0  0  0  0]\n",
            " [ 0 15  0  0  0  0]\n",
            " [ 0  0  3  0  0  0]\n",
            " [ 0  0  0  3  0  0]\n",
            " [ 0  0  0  0  2  0]\n",
            " [ 0  0  0  0  0  6]]\n",
            "\u001b[1m\n",
            " Accuracy of ANN: 1.0\n",
            "\n",
            "\u001b[0m\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       1.00      1.00      1.00        14\n",
            "         2.0       1.00      1.00      1.00        15\n",
            "         3.0       1.00      1.00      1.00         3\n",
            "         5.0       1.00      1.00      1.00         3\n",
            "         6.0       1.00      1.00      1.00         2\n",
            "         7.0       1.00      1.00      1.00         6\n",
            "\n",
            "    accuracy                           1.00        43\n",
            "   macro avg       1.00      1.00      1.00        43\n",
            "weighted avg       1.00      1.00      1.00        43\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "הרחיבו והתאימו את המנוע הגנטי שלכם לפי העקרונות שתוארו בהרצאה כדי לייצר רשתות נוירונים עמוקות"
      ],
      "metadata": {
        "id": "5u2L5vZcLCwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from the Lecture:\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# sigmoid and ReLU function\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "def ReLU(x):\n",
        "    return x * ( x > 0)"
      ],
      "metadata": {
        "id": "zivw8hc2Lznr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the genitic algorithm from the lecture modified \n",
        "\n",
        "class genetic_algorithm:\n",
        "    def execute(pop_size,generations,threshold,x,y,network):\n",
        "        class Agent:\n",
        "            def __init__(self,network):\n",
        "                class neural_network:\n",
        "                    def __init__(self,network):\n",
        "                        self.depth = []\n",
        "                        self.weights = []\n",
        "                        self.activations = []\n",
        "                        for layer in network:\n",
        "                            if layer[0]!= None:\n",
        "                                input_size=layer[0]\n",
        "                            else:\n",
        "                                input_size=network[network.index(layer)-1][1]\n",
        "                            output_size = layer[1]\n",
        "                            activation = layer[2]\n",
        "                            self.weights.append(np.random.randn(input_size,output_size))\n",
        "                            self.activations.append(activation)\n",
        "                    def propagate(self,data):\n",
        "                        input_data = data\n",
        "                        for i in range(len(self.weights)):\n",
        "                            z=np.dot(input_data,self.weights[i])\n",
        "                            a=self.activations[i](z)\n",
        "                            input_data=a\n",
        "                        yhat=a\n",
        "                        return yhat\n",
        "                self.neural_network = neural_network(network)\n",
        "                self.fitness = 0\n",
        "            def __str__(self):\n",
        "                    return 'Loss: ' + str(self.fitness[0])\n",
        "\n",
        "        def generate_agents(population, network):\n",
        "\n",
        "            return [Agent(network) for _ in range(population)]\n",
        "\n",
        "        def fitness(agents,x,y):\n",
        "            for agent in agents:\n",
        "                yhat = agent.neural_network.propagate(x)\n",
        "                cost = (yhat-y)**2\n",
        "                agent.fitness=sum(cost)\n",
        "            return agents\n",
        "\n",
        "        def selection(agents):\n",
        "            agents = sorted(agents, key=lambda agent: agent.fitness,reverse=False)\n",
        "            print('\\n'.join(map(str,agents)))\n",
        "            agents = agents[:int(0.2 * len(agents))]\n",
        "            return agents\n",
        "\n",
        "        def unflatten(flattened,shapes):\n",
        "            newarray = []\n",
        "            index = 0\n",
        "            for shape in shapes:\n",
        "                size = np.product(shape)\n",
        "                newarray.append(flattened[index : index + size].reshape(shape))\n",
        "                index += size\n",
        "            return newarray\n",
        "\n",
        "        def crossover(agents,network,pop_size):\n",
        "            offspring=[]\n",
        "            for _ in range((pop_size- len(agents)) // 2):\n",
        "                parent1 = random.choice(agents)\n",
        "                parent2 = random.choice(agents)\n",
        "                child1 = Agent(network)\n",
        "                child2 = Agent(network)\n",
        "\n",
        "                shapes = [a.shape for a in parent1.neural_network.weights]\n",
        "                genes1 = np.concatenate([a.flatten() for a in parent1.neural_network.weights])\n",
        "                genes2 = np.concatenate([a.flatten() for a in parent2.neural_network.weights])\n",
        "\n",
        "                split = random.randint(0, len(genes1)-1)\n",
        "                child1_genes = np.array(genes1[0:split].tolist() + genes2[split:].tolist())\n",
        "                child2_genes = np.array(genes1[0:split].tolist() + genes2[split:].tolist())\n",
        "\n",
        "                child1.neural_network.weights = unflatten(child1_genes, shapes)\n",
        "                child2.neural_network.weights = unflatten(child2_genes, shapes)\n",
        "                offspring.append(child1)\n",
        "                offspring.append(child2)\n",
        "            agents.extend(offspring)\n",
        "            return agents\n",
        "\n",
        "\n",
        "        def mutation(agents):\n",
        "            for agent in agents:\n",
        "                if random.uniform(0.0,1.0) <= 0.1:\n",
        "                    weights = agent.neural_network.weights\n",
        "                    shapes = [a.shape for a in weights]\n",
        "                    flattened = np.concatenate([a.flatten() for a in weights])\n",
        "                    randint = random.randint(0,len(flattened)-1)\n",
        "                    flattened[randint] = np.random.randn()\n",
        "                    newarray = []\n",
        "                    indeweights = 0\n",
        "                    for shape in shapes:\n",
        "                        size=np.product(shape)\n",
        "                        newarray.append(flattened[indeweights : indeweights + size].reshape(shape))\n",
        "                        indeweights += size\n",
        "                    agent.neural_network.weights = newarray\n",
        "                    return agents\n",
        "    \n",
        "\n",
        "\n",
        "        for i in range(generations):\n",
        "            print('Generation',str(i),':')\n",
        "            agents = generate_agents(pop_size,network)\n",
        "            agents = fitness(agents,x,y)\n",
        "            agents = selection(agents)\n",
        "            agents = crossover(agents,network,pop_size)\n",
        "            agents = mutation(agents)\n",
        "            agents = fitness(agents,x,y)\n",
        "\n",
        "            if any(agent.fitness < threshold for agent in agents):\n",
        "                print('Threshold met at generation' + str(i) + '!')\n",
        "            if i % 100:\n",
        "                clear_output()\n",
        "\n",
        "        return agents[0]"
      ],
      "metadata": {
        "id": "DBevihbyMla7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trying to run the code from the lecture:\n",
        "\n",
        "x = np.array([[0,0,1],[1,1,1],[1,0,1],[0,1,1]])\n",
        "y = np.array([[0,1,1,0]]).T\n",
        "\n",
        "p = random.randint(0, 1)\n",
        "if p == 0:\n",
        "    activ = ReLU\n",
        "else:\n",
        "    activ = sigmoid\n",
        "\n",
        "network = [[3,10,activ],[None,1,activ]]\n",
        "#x = np.array(x_train)\n",
        "#y = np.array(y_train[:, np.newaxis])\n",
        "#network = [[11,10,activ],[None,6,activ]]\n",
        "ga = genetic_algorithm\n",
        "agent = ga.execute(100,100,0.1,x,y,network)\n",
        "weights = agent.neural_network.weights\n",
        "print(agent.fitness)\n",
        "agent.neural_network.propagate(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSYbk6iYMuYK",
        "outputId": "e32c056c-bcd3-4dbd-a2d9-5d8a52648001"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.15416681]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.12986551],\n",
              "       [ 0.62976884],\n",
              "       [ 0.98481313],\n",
              "       [-0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyMjanjsZJSb",
        "outputId": "1f3261c4-49b3-4944-96d0-92386dde864c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.47.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgCet-Mnh2yV",
        "outputId": "cc87cd79-708f-4acc-b97a-ed4d967ef91a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting glass.csv\n"
          ]
        }
      ],
      "source": [
        "%%writefile glass.csv\n",
        "1,1.52101,13.64,4.49,1.10,71.78,0.06,8.75,0.00,0.00,1\n",
        "2,1.51761,13.89,3.60,1.36,72.73,0.48,7.83,0.00,0.00,1\n",
        "3,1.51618,13.53,3.55,1.54,72.99,0.39,7.78,0.00,0.00,1\n",
        "4,1.51766,13.21,3.69,1.29,72.61,0.57,8.22,0.00,0.00,1\n",
        "5,1.51742,13.27,3.62,1.24,73.08,0.55,8.07,0.00,0.00,1\n",
        "6,1.51596,12.79,3.61,1.62,72.97,0.64,8.07,0.00,0.26,1\n",
        "7,1.51743,13.30,3.60,1.14,73.09,0.58,8.17,0.00,0.00,1\n",
        "8,1.51756,13.15,3.61,1.05,73.24,0.57,8.24,0.00,0.00,1\n",
        "9,1.51918,14.04,3.58,1.37,72.08,0.56,8.30,0.00,0.00,1\n",
        "10,1.51755,13.00,3.60,1.36,72.99,0.57,8.40,0.00,0.11,1\n",
        "11,1.51571,12.72,3.46,1.56,73.20,0.67,8.09,0.00,0.24,1\n",
        "12,1.51763,12.80,3.66,1.27,73.01,0.60,8.56,0.00,0.00,1\n",
        "13,1.51589,12.88,3.43,1.40,73.28,0.69,8.05,0.00,0.24,1\n",
        "14,1.51748,12.86,3.56,1.27,73.21,0.54,8.38,0.00,0.17,1\n",
        "15,1.51763,12.61,3.59,1.31,73.29,0.58,8.50,0.00,0.00,1\n",
        "16,1.51761,12.81,3.54,1.23,73.24,0.58,8.39,0.00,0.00,1\n",
        "17,1.51784,12.68,3.67,1.16,73.11,0.61,8.70,0.00,0.00,1\n",
        "18,1.52196,14.36,3.85,0.89,71.36,0.15,9.15,0.00,0.00,1\n",
        "19,1.51911,13.90,3.73,1.18,72.12,0.06,8.89,0.00,0.00,1\n",
        "20,1.51735,13.02,3.54,1.69,72.73,0.54,8.44,0.00,0.07,1\n",
        "21,1.51750,12.82,3.55,1.49,72.75,0.54,8.52,0.00,0.19,1\n",
        "22,1.51966,14.77,3.75,0.29,72.02,0.03,9.00,0.00,0.00,1\n",
        "23,1.51736,12.78,3.62,1.29,72.79,0.59,8.70,0.00,0.00,1\n",
        "24,1.51751,12.81,3.57,1.35,73.02,0.62,8.59,0.00,0.00,1\n",
        "25,1.51720,13.38,3.50,1.15,72.85,0.50,8.43,0.00,0.00,1\n",
        "26,1.51764,12.98,3.54,1.21,73.00,0.65,8.53,0.00,0.00,1\n",
        "27,1.51793,13.21,3.48,1.41,72.64,0.59,8.43,0.00,0.00,1\n",
        "28,1.51721,12.87,3.48,1.33,73.04,0.56,8.43,0.00,0.00,1\n",
        "29,1.51768,12.56,3.52,1.43,73.15,0.57,8.54,0.00,0.00,1\n",
        "30,1.51784,13.08,3.49,1.28,72.86,0.60,8.49,0.00,0.00,1\n",
        "31,1.51768,12.65,3.56,1.30,73.08,0.61,8.69,0.00,0.14,1\n",
        "32,1.51747,12.84,3.50,1.14,73.27,0.56,8.55,0.00,0.00,1\n",
        "33,1.51775,12.85,3.48,1.23,72.97,0.61,8.56,0.09,0.22,1\n",
        "34,1.51753,12.57,3.47,1.38,73.39,0.60,8.55,0.00,0.06,1\n",
        "35,1.51783,12.69,3.54,1.34,72.95,0.57,8.75,0.00,0.00,1\n",
        "36,1.51567,13.29,3.45,1.21,72.74,0.56,8.57,0.00,0.00,1\n",
        "37,1.51909,13.89,3.53,1.32,71.81,0.51,8.78,0.11,0.00,1\n",
        "38,1.51797,12.74,3.48,1.35,72.96,0.64,8.68,0.00,0.00,1\n",
        "39,1.52213,14.21,3.82,0.47,71.77,0.11,9.57,0.00,0.00,1\n",
        "40,1.52213,14.21,3.82,0.47,71.77,0.11,9.57,0.00,0.00,1\n",
        "41,1.51793,12.79,3.50,1.12,73.03,0.64,8.77,0.00,0.00,1\n",
        "42,1.51755,12.71,3.42,1.20,73.20,0.59,8.64,0.00,0.00,1\n",
        "43,1.51779,13.21,3.39,1.33,72.76,0.59,8.59,0.00,0.00,1\n",
        "44,1.52210,13.73,3.84,0.72,71.76,0.17,9.74,0.00,0.00,1\n",
        "45,1.51786,12.73,3.43,1.19,72.95,0.62,8.76,0.00,0.30,1\n",
        "46,1.51900,13.49,3.48,1.35,71.95,0.55,9.00,0.00,0.00,1\n",
        "47,1.51869,13.19,3.37,1.18,72.72,0.57,8.83,0.00,0.16,1\n",
        "48,1.52667,13.99,3.70,0.71,71.57,0.02,9.82,0.00,0.10,1\n",
        "49,1.52223,13.21,3.77,0.79,71.99,0.13,10.02,0.00,0.00,1\n",
        "50,1.51898,13.58,3.35,1.23,72.08,0.59,8.91,0.00,0.00,1\n",
        "51,1.52320,13.72,3.72,0.51,71.75,0.09,10.06,0.00,0.16,1\n",
        "52,1.51926,13.20,3.33,1.28,72.36,0.60,9.14,0.00,0.11,1\n",
        "53,1.51808,13.43,2.87,1.19,72.84,0.55,9.03,0.00,0.00,1\n",
        "54,1.51837,13.14,2.84,1.28,72.85,0.55,9.07,0.00,0.00,1\n",
        "55,1.51778,13.21,2.81,1.29,72.98,0.51,9.02,0.00,0.09,1\n",
        "56,1.51769,12.45,2.71,1.29,73.70,0.56,9.06,0.00,0.24,1\n",
        "57,1.51215,12.99,3.47,1.12,72.98,0.62,8.35,0.00,0.31,1\n",
        "58,1.51824,12.87,3.48,1.29,72.95,0.60,8.43,0.00,0.00,1\n",
        "59,1.51754,13.48,3.74,1.17,72.99,0.59,8.03,0.00,0.00,1\n",
        "60,1.51754,13.39,3.66,1.19,72.79,0.57,8.27,0.00,0.11,1\n",
        "61,1.51905,13.60,3.62,1.11,72.64,0.14,8.76,0.00,0.00,1\n",
        "62,1.51977,13.81,3.58,1.32,71.72,0.12,8.67,0.69,0.00,1\n",
        "63,1.52172,13.51,3.86,0.88,71.79,0.23,9.54,0.00,0.11,1\n",
        "64,1.52227,14.17,3.81,0.78,71.35,0.00,9.69,0.00,0.00,1\n",
        "65,1.52172,13.48,3.74,0.90,72.01,0.18,9.61,0.00,0.07,1\n",
        "66,1.52099,13.69,3.59,1.12,71.96,0.09,9.40,0.00,0.00,1\n",
        "67,1.52152,13.05,3.65,0.87,72.22,0.19,9.85,0.00,0.17,1\n",
        "68,1.52152,13.05,3.65,0.87,72.32,0.19,9.85,0.00,0.17,1\n",
        "69,1.52152,13.12,3.58,0.90,72.20,0.23,9.82,0.00,0.16,1\n",
        "70,1.52300,13.31,3.58,0.82,71.99,0.12,10.17,0.00,0.03,1\n",
        "71,1.51574,14.86,3.67,1.74,71.87,0.16,7.36,0.00,0.12,2\n",
        "72,1.51848,13.64,3.87,1.27,71.96,0.54,8.32,0.00,0.32,2\n",
        "73,1.51593,13.09,3.59,1.52,73.10,0.67,7.83,0.00,0.00,2\n",
        "74,1.51631,13.34,3.57,1.57,72.87,0.61,7.89,0.00,0.00,2\n",
        "75,1.51596,13.02,3.56,1.54,73.11,0.72,7.90,0.00,0.00,2\n",
        "76,1.51590,13.02,3.58,1.51,73.12,0.69,7.96,0.00,0.00,2\n",
        "77,1.51645,13.44,3.61,1.54,72.39,0.66,8.03,0.00,0.00,2\n",
        "78,1.51627,13.00,3.58,1.54,72.83,0.61,8.04,0.00,0.00,2\n",
        "79,1.51613,13.92,3.52,1.25,72.88,0.37,7.94,0.00,0.14,2\n",
        "80,1.51590,12.82,3.52,1.90,72.86,0.69,7.97,0.00,0.00,2\n",
        "81,1.51592,12.86,3.52,2.12,72.66,0.69,7.97,0.00,0.00,2\n",
        "82,1.51593,13.25,3.45,1.43,73.17,0.61,7.86,0.00,0.00,2\n",
        "83,1.51646,13.41,3.55,1.25,72.81,0.68,8.10,0.00,0.00,2\n",
        "84,1.51594,13.09,3.52,1.55,72.87,0.68,8.05,0.00,0.09,2\n",
        "85,1.51409,14.25,3.09,2.08,72.28,1.10,7.08,0.00,0.00,2\n",
        "86,1.51625,13.36,3.58,1.49,72.72,0.45,8.21,0.00,0.00,2\n",
        "87,1.51569,13.24,3.49,1.47,73.25,0.38,8.03,0.00,0.00,2\n",
        "88,1.51645,13.40,3.49,1.52,72.65,0.67,8.08,0.00,0.10,2\n",
        "89,1.51618,13.01,3.50,1.48,72.89,0.60,8.12,0.00,0.00,2\n",
        "90,1.51640,12.55,3.48,1.87,73.23,0.63,8.08,0.00,0.09,2\n",
        "91,1.51841,12.93,3.74,1.11,72.28,0.64,8.96,0.00,0.22,2\n",
        "92,1.51605,12.90,3.44,1.45,73.06,0.44,8.27,0.00,0.00,2\n",
        "93,1.51588,13.12,3.41,1.58,73.26,0.07,8.39,0.00,0.19,2\n",
        "94,1.51590,13.24,3.34,1.47,73.10,0.39,8.22,0.00,0.00,2\n",
        "95,1.51629,12.71,3.33,1.49,73.28,0.67,8.24,0.00,0.00,2\n",
        "96,1.51860,13.36,3.43,1.43,72.26,0.51,8.60,0.00,0.00,2\n",
        "97,1.51841,13.02,3.62,1.06,72.34,0.64,9.13,0.00,0.15,2\n",
        "98,1.51743,12.20,3.25,1.16,73.55,0.62,8.90,0.00,0.24,2\n",
        "99,1.51689,12.67,2.88,1.71,73.21,0.73,8.54,0.00,0.00,2\n",
        "100,1.51811,12.96,2.96,1.43,72.92,0.60,8.79,0.14,0.00,2\n",
        "101,1.51655,12.75,2.85,1.44,73.27,0.57,8.79,0.11,0.22,2\n",
        "102,1.51730,12.35,2.72,1.63,72.87,0.70,9.23,0.00,0.00,2\n",
        "103,1.51820,12.62,2.76,0.83,73.81,0.35,9.42,0.00,0.20,2\n",
        "104,1.52725,13.80,3.15,0.66,70.57,0.08,11.64,0.00,0.00,2\n",
        "105,1.52410,13.83,2.90,1.17,71.15,0.08,10.79,0.00,0.00,2\n",
        "106,1.52475,11.45,0.00,1.88,72.19,0.81,13.24,0.00,0.34,2\n",
        "107,1.53125,10.73,0.00,2.10,69.81,0.58,13.30,3.15,0.28,2\n",
        "108,1.53393,12.30,0.00,1.00,70.16,0.12,16.19,0.00,0.24,2\n",
        "109,1.52222,14.43,0.00,1.00,72.67,0.10,11.52,0.00,0.08,2\n",
        "110,1.51818,13.72,0.00,0.56,74.45,0.00,10.99,0.00,0.00,2\n",
        "111,1.52664,11.23,0.00,0.77,73.21,0.00,14.68,0.00,0.00,2\n",
        "112,1.52739,11.02,0.00,0.75,73.08,0.00,14.96,0.00,0.00,2\n",
        "113,1.52777,12.64,0.00,0.67,72.02,0.06,14.40,0.00,0.00,2\n",
        "114,1.51892,13.46,3.83,1.26,72.55,0.57,8.21,0.00,0.14,2\n",
        "115,1.51847,13.10,3.97,1.19,72.44,0.60,8.43,0.00,0.00,2\n",
        "116,1.51846,13.41,3.89,1.33,72.38,0.51,8.28,0.00,0.00,2\n",
        "117,1.51829,13.24,3.90,1.41,72.33,0.55,8.31,0.00,0.10,2\n",
        "118,1.51708,13.72,3.68,1.81,72.06,0.64,7.88,0.00,0.00,2\n",
        "119,1.51673,13.30,3.64,1.53,72.53,0.65,8.03,0.00,0.29,2\n",
        "120,1.51652,13.56,3.57,1.47,72.45,0.64,7.96,0.00,0.00,2\n",
        "121,1.51844,13.25,3.76,1.32,72.40,0.58,8.42,0.00,0.00,2\n",
        "122,1.51663,12.93,3.54,1.62,72.96,0.64,8.03,0.00,0.21,2\n",
        "123,1.51687,13.23,3.54,1.48,72.84,0.56,8.10,0.00,0.00,2\n",
        "124,1.51707,13.48,3.48,1.71,72.52,0.62,7.99,0.00,0.00,2\n",
        "125,1.52177,13.20,3.68,1.15,72.75,0.54,8.52,0.00,0.00,2\n",
        "126,1.51872,12.93,3.66,1.56,72.51,0.58,8.55,0.00,0.12,2\n",
        "127,1.51667,12.94,3.61,1.26,72.75,0.56,8.60,0.00,0.00,2\n",
        "128,1.52081,13.78,2.28,1.43,71.99,0.49,9.85,0.00,0.17,2\n",
        "129,1.52068,13.55,2.09,1.67,72.18,0.53,9.57,0.27,0.17,2\n",
        "130,1.52020,13.98,1.35,1.63,71.76,0.39,10.56,0.00,0.18,2\n",
        "131,1.52177,13.75,1.01,1.36,72.19,0.33,11.14,0.00,0.00,2\n",
        "132,1.52614,13.70,0.00,1.36,71.24,0.19,13.44,0.00,0.10,2\n",
        "133,1.51813,13.43,3.98,1.18,72.49,0.58,8.15,0.00,0.00,2\n",
        "134,1.51800,13.71,3.93,1.54,71.81,0.54,8.21,0.00,0.15,2\n",
        "135,1.51811,13.33,3.85,1.25,72.78,0.52,8.12,0.00,0.00,2\n",
        "136,1.51789,13.19,3.90,1.30,72.33,0.55,8.44,0.00,0.28,2\n",
        "137,1.51806,13.00,3.80,1.08,73.07,0.56,8.38,0.00,0.12,2\n",
        "138,1.51711,12.89,3.62,1.57,72.96,0.61,8.11,0.00,0.00,2\n",
        "139,1.51674,12.79,3.52,1.54,73.36,0.66,7.90,0.00,0.00,2\n",
        "140,1.51674,12.87,3.56,1.64,73.14,0.65,7.99,0.00,0.00,2\n",
        "141,1.51690,13.33,3.54,1.61,72.54,0.68,8.11,0.00,0.00,2\n",
        "142,1.51851,13.20,3.63,1.07,72.83,0.57,8.41,0.09,0.17,2\n",
        "143,1.51662,12.85,3.51,1.44,73.01,0.68,8.23,0.06,0.25,2\n",
        "144,1.51709,13.00,3.47,1.79,72.72,0.66,8.18,0.00,0.00,2\n",
        "145,1.51660,12.99,3.18,1.23,72.97,0.58,8.81,0.00,0.24,2\n",
        "146,1.51839,12.85,3.67,1.24,72.57,0.62,8.68,0.00,0.35,2\n",
        "147,1.51769,13.65,3.66,1.11,72.77,0.11,8.60,0.00,0.00,3\n",
        "148,1.51610,13.33,3.53,1.34,72.67,0.56,8.33,0.00,0.00,3\n",
        "149,1.51670,13.24,3.57,1.38,72.70,0.56,8.44,0.00,0.10,3\n",
        "150,1.51643,12.16,3.52,1.35,72.89,0.57,8.53,0.00,0.00,3\n",
        "151,1.51665,13.14,3.45,1.76,72.48,0.60,8.38,0.00,0.17,3\n",
        "152,1.52127,14.32,3.90,0.83,71.50,0.00,9.49,0.00,0.00,3\n",
        "153,1.51779,13.64,3.65,0.65,73.00,0.06,8.93,0.00,0.00,3\n",
        "154,1.51610,13.42,3.40,1.22,72.69,0.59,8.32,0.00,0.00,3\n",
        "155,1.51694,12.86,3.58,1.31,72.61,0.61,8.79,0.00,0.00,3\n",
        "156,1.51646,13.04,3.40,1.26,73.01,0.52,8.58,0.00,0.00,3\n",
        "157,1.51655,13.41,3.39,1.28,72.64,0.52,8.65,0.00,0.00,3\n",
        "158,1.52121,14.03,3.76,0.58,71.79,0.11,9.65,0.00,0.00,3\n",
        "159,1.51776,13.53,3.41,1.52,72.04,0.58,8.79,0.00,0.00,3\n",
        "160,1.51796,13.50,3.36,1.63,71.94,0.57,8.81,0.00,0.09,3\n",
        "161,1.51832,13.33,3.34,1.54,72.14,0.56,8.99,0.00,0.00,3\n",
        "162,1.51934,13.64,3.54,0.75,72.65,0.16,8.89,0.15,0.24,3\n",
        "163,1.52211,14.19,3.78,0.91,71.36,0.23,9.14,0.00,0.37,3\n",
        "164,1.51514,14.01,2.68,3.50,69.89,1.68,5.87,2.20,0.00,5\n",
        "165,1.51915,12.73,1.85,1.86,72.69,0.60,10.09,0.00,0.00,5\n",
        "166,1.52171,11.56,1.88,1.56,72.86,0.47,11.41,0.00,0.00,5\n",
        "167,1.52151,11.03,1.71,1.56,73.44,0.58,11.62,0.00,0.00,5\n",
        "168,1.51969,12.64,0.00,1.65,73.75,0.38,11.53,0.00,0.00,5\n",
        "169,1.51666,12.86,0.00,1.83,73.88,0.97,10.17,0.00,0.00,5\n",
        "170,1.51994,13.27,0.00,1.76,73.03,0.47,11.32,0.00,0.00,5\n",
        "171,1.52369,13.44,0.00,1.58,72.22,0.32,12.24,0.00,0.00,5\n",
        "172,1.51316,13.02,0.00,3.04,70.48,6.21,6.96,0.00,0.00,5\n",
        "173,1.51321,13.00,0.00,3.02,70.70,6.21,6.93,0.00,0.00,5\n",
        "174,1.52043,13.38,0.00,1.40,72.25,0.33,12.50,0.00,0.00,5\n",
        "175,1.52058,12.85,1.61,2.17,72.18,0.76,9.70,0.24,0.51,5\n",
        "176,1.52119,12.97,0.33,1.51,73.39,0.13,11.27,0.00,0.28,5\n",
        "177,1.51905,14.00,2.39,1.56,72.37,0.00,9.57,0.00,0.00,6\n",
        "178,1.51937,13.79,2.41,1.19,72.76,0.00,9.77,0.00,0.00,6\n",
        "179,1.51829,14.46,2.24,1.62,72.38,0.00,9.26,0.00,0.00,6\n",
        "180,1.51852,14.09,2.19,1.66,72.67,0.00,9.32,0.00,0.00,6\n",
        "181,1.51299,14.40,1.74,1.54,74.55,0.00,7.59,0.00,0.00,6\n",
        "182,1.51888,14.99,0.78,1.74,72.50,0.00,9.95,0.00,0.00,6\n",
        "183,1.51916,14.15,0.00,2.09,72.74,0.00,10.88,0.00,0.00,6\n",
        "184,1.51969,14.56,0.00,0.56,73.48,0.00,11.22,0.00,0.00,6\n",
        "185,1.51115,17.38,0.00,0.34,75.41,0.00,6.65,0.00,0.00,6\n",
        "186,1.51131,13.69,3.20,1.81,72.81,1.76,5.43,1.19,0.00,7\n",
        "187,1.51838,14.32,3.26,2.22,71.25,1.46,5.79,1.63,0.00,7\n",
        "188,1.52315,13.44,3.34,1.23,72.38,0.60,8.83,0.00,0.00,7\n",
        "189,1.52247,14.86,2.20,2.06,70.26,0.76,9.76,0.00,0.00,7\n",
        "190,1.52365,15.79,1.83,1.31,70.43,0.31,8.61,1.68,0.00,7\n",
        "191,1.51613,13.88,1.78,1.79,73.10,0.00,8.67,0.76,0.00,7\n",
        "192,1.51602,14.85,0.00,2.38,73.28,0.00,8.76,0.64,0.09,7\n",
        "193,1.51623,14.20,0.00,2.79,73.46,0.04,9.04,0.40,0.09,7\n",
        "194,1.51719,14.75,0.00,2.00,73.02,0.00,8.53,1.59,0.08,7\n",
        "195,1.51683,14.56,0.00,1.98,73.29,0.00,8.52,1.57,0.07,7\n",
        "196,1.51545,14.14,0.00,2.68,73.39,0.08,9.07,0.61,0.05,7\n",
        "197,1.51556,13.87,0.00,2.54,73.23,0.14,9.41,0.81,0.01,7\n",
        "198,1.51727,14.70,0.00,2.34,73.28,0.00,8.95,0.66,0.00,7\n",
        "199,1.51531,14.38,0.00,2.66,73.10,0.04,9.08,0.64,0.00,7\n",
        "200,1.51609,15.01,0.00,2.51,73.05,0.05,8.83,0.53,0.00,7\n",
        "201,1.51508,15.15,0.00,2.25,73.50,0.00,8.34,0.63,0.00,7\n",
        "202,1.51653,11.95,0.00,1.19,75.18,2.70,8.93,0.00,0.00,7\n",
        "203,1.51514,14.85,0.00,2.42,73.72,0.00,8.39,0.56,0.00,7\n",
        "204,1.51658,14.80,0.00,1.99,73.11,0.00,8.28,1.71,0.00,7\n",
        "205,1.51617,14.95,0.00,2.27,73.30,0.00,8.71,0.67,0.00,7\n",
        "206,1.51732,14.95,0.00,1.80,72.99,0.00,8.61,1.55,0.00,7\n",
        "207,1.51645,14.94,0.00,1.87,73.11,0.00,8.67,1.38,0.00,7\n",
        "208,1.51831,14.39,0.00,1.82,72.86,1.41,6.47,2.88,0.00,7\n",
        "209,1.51640,14.37,0.00,2.74,72.85,0.00,9.45,0.54,0.00,7\n",
        "210,1.51623,14.14,0.00,2.88,72.61,0.08,9.18,1.06,0.00,7\n",
        "211,1.51685,14.92,0.00,1.99,73.06,0.00,8.40,1.59,0.00,7\n",
        "212,1.52065,14.36,0.00,2.02,73.42,0.00,8.44,1.64,0.00,7\n",
        "213,1.51651,14.38,0.00,1.94,73.61,0.00,8.48,1.57,0.00,7\n",
        "214,1.51711,14.23,0.00,2.08,73.36,0.00,8.62,1.67,0.00,7\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#trying with a different code : works but we coudn't get sigmoid activation to run instead of tanh\n",
        "\n",
        "import sys\n",
        "from copy import deepcopy, copy\n",
        "POPSIZE = 8\n",
        "Iterations = 9\n",
        "ELITRATE = 0.1\n",
        "\n",
        "# sigmoid and ReLU function\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "def ReLU(x):\n",
        "    return x * ( x > 0)\n",
        "\n",
        "class DS:\n",
        "\n",
        "\n",
        "    def retrieveDataset(self):\n",
        "        f = open('glass.csv', 'r')\n",
        "        #f = df.read().decode()\n",
        "\n",
        "        nn_input = []\n",
        "        nn_output = []\n",
        "\n",
        "        for line in f:\n",
        "            elements = line.split(',')\n",
        "            df.columns = ['', '', '', '', '', '','', '', '', '', 'GlassType']\n",
        "            elements = df.columns\n",
        "            nn_input.append([float(x) for x in elements[1:-1]])\n",
        "            dict = {1: 0, 2: 1, 3: 2, 5: 3, 6: 4, 7: 5}\n",
        "            nn_output.append(dict[int(elements[-1][0])])\n",
        "\n",
        "        return nn_input, nn_output\n",
        "\n",
        "    def divideDataset(self, nn_input, nn_output):\n",
        "\n",
        "        classes = [[], [], [], [], [], []]\n",
        "\n",
        "        train_input, train_output, test_input, test_output = [], [], [], []\n",
        "\n",
        "        for i in range(len(nn_input)):\n",
        "            classes[nn_output[i]].append(nn_input[i])\n",
        "\n",
        "        for i in range(6):\n",
        "\n",
        "            test_size = int(0.2 * len(classes[i]))\n",
        "            for j in range(test_size):\n",
        "                test_input.append(classes[i][j])\n",
        "                test_output.append(i)\n",
        "\n",
        "            for j in range(test_size + 1, len(classes[i])):\n",
        "                train_input.append(classes[i][j])\n",
        "                train_output.append(i)\n",
        "\n",
        "        return train_input, train_output, test_input, test_output\n",
        "\n",
        "    def normalizeDataset(self, nn_input):\n",
        "\n",
        "        for j in range(9):\n",
        "            f_min, f_max = 100.0, 0.00\n",
        "            f_max = max(nn_input[:][j])\n",
        "\n",
        "            for i in range(len(nn_input)):\n",
        "                f_min = min(f_min, nn_input[i][j])\n",
        "                f_max = max(f_max, nn_input[i][j])\n",
        "\n",
        "            for i in range(len(nn_input)):\n",
        "                nn_input[i][j] = (nn_input[i][j] - f_min) / (f_max - f_min)\n",
        "\n",
        "        return nn_input\n",
        "\n",
        "    def preprocessData(self):\n",
        "        nn_input, nn_output = self.retrieveDataset()\n",
        "        nn_input = self.normalizeDataset(nn_input)\n",
        "        train_input, train_output, test_input, test_output = self.divideDataset(nn_input, nn_output)\n",
        "        return train_input, train_output, test_input, test_output\n",
        "\n",
        "\n",
        "class Network:\n",
        "\n",
        "    def __init__(self, depth, hidden, activation):\n",
        "        self.depth = depth\n",
        "        self.hidden = hidden\n",
        "        self.activation = activation\n",
        "\n",
        "\n",
        "class GeneticStruct:\n",
        "\n",
        "    def __init__(self, network, fitness, reg):\n",
        "        self.network = network\n",
        "        self.fitness = fitness\n",
        "        self.reg = reg\n",
        "\n",
        "\n",
        "class InitPop:\n",
        "\n",
        "    def initPopulation(self):\n",
        "\n",
        "        population = []\n",
        "\n",
        "        for i in range(POPSIZE):\n",
        "            l = []\n",
        "            r = random.randrange(1, 10)\n",
        "            depth = r\n",
        "            for i in range(depth):\n",
        "                tmp = random.randrange(2, 200)\n",
        "                l.append(tmp)\n",
        "\n",
        "            prob = random.randint(0, 1)\n",
        "            if prob == 0:\n",
        "                activation = 'relu'\n",
        "            else:\n",
        "                activation = 'tanh'\n",
        "\n",
        "            fitness = np.inf\n",
        "            curr_network = Network(depth, l, activation)\n",
        "\n",
        "            agents = GeneticStruct(curr_network, fitness, 0)\n",
        "            population.append(agents)\n",
        "\n",
        "        return population\n",
        "\n",
        "\n",
        "    def calculateReg(self, cls, agents):\n",
        "\n",
        "        c = 1\n",
        "        l = 0.5\n",
        "\n",
        "        weights = cls.coefs_\n",
        "        length = len(weights)\n",
        "        weight_sum = 0\n",
        "\n",
        "        for i in range(length):\n",
        "            helper = len(weights[i])\n",
        "\n",
        "            for j in range(helper):\n",
        "                print(weights[i][j])\n",
        "                curr = weights[i][j] ** 2\n",
        "                weight_sum = weight_sum + curr\n",
        "\n",
        "\n",
        "        depth = agents.network.depth\n",
        "\n",
        "        for k in range(depth):\n",
        "            d = agents.network.hidden[k]\n",
        "            c = c * d\n",
        "\n",
        "        c = c * 54\n",
        "        x1 = weight_sum * l\n",
        "        x2 = len(x_train) * 2\n",
        "        reg = x1 / x2 / c\n",
        "\n",
        "        return reg\n",
        "\n",
        "\n",
        "    def calcFitnessOneItr(self, agents):\n",
        "        h = agents.network.hidden\n",
        "        itr = 3000\n",
        "        a = agents.network.activation\n",
        "        s = 'adam'\n",
        "\n",
        "        reg = 0\n",
        "\n",
        "        cls = neural_network.MLPClassifier(hidden_layer_sizes=h, max_iter=itr, activation=a, solver=s, random_state=1)\n",
        "\n",
        "        cls.fit(x_train, y_train)\n",
        "        predictionY = cls.predict(x_test)\n",
        "\n",
        "        confusion = confusion_matrix(predictionY, y_test)\n",
        "        overall_sum = confusion.sum()\n",
        "        d_sum = confusion.trace()\n",
        "\n",
        "        agents.fitness = d_sum / overall_sum\n",
        "\n",
        "\n",
        "        #reg = self.calculateReg(cls, agents)\n",
        "        agents.reg = reg\n",
        "\n",
        "        return agents\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def calculateFittnes(self, population):\n",
        "\n",
        "        for i in range(len(population)):\n",
        "            population[i] = self.calcFitnessOneItr(population[i])\n",
        "\n",
        "        return population\n",
        "\n",
        "\n",
        "    def sortFitness(self, population):\n",
        "\n",
        "        population.sort(key=lambda x: - x.fitness)\n",
        "\n",
        "        return population\n",
        "\n",
        "    def elitism(self, population: list, buffer: list, esize):\n",
        "\n",
        "        temp = population[:esize].copy()\n",
        "        buffer[:esize] = temp\n",
        "\n",
        "        return buffer\n",
        "\n",
        "    def mate(self, population, buffer):\n",
        "\n",
        "        esize = int(POPSIZE * ELITRATE)\n",
        "        buffer = self.elitism(population, buffer, esize)\n",
        "\n",
        "        # mate the rest\n",
        "        for i in range(esize, POPSIZE):\n",
        "            i1 = random.randrange(0, POPSIZE - 1)\n",
        "            i2 = random.randrange(0, POPSIZE - 1)\n",
        "            print(len(population))\n",
        "            print(i1)\n",
        "            print(i2)\n",
        "\n",
        "            pop1 = population[i1]\n",
        "            pop2 = population[i2]\n",
        "\n",
        "            index1 = random.randrange(0, pop1.network.depth)\n",
        "            index2 = random.randrange(0, pop2.network.depth)\n",
        "\n",
        "            buffer[i].network.hidden = pop1.network.hidden[:index1] + pop2.network.hidden[index1:index2] + pop1.network.hidden[index2:]\n",
        "            buffer[i].network.depth = len(buffer[i].network.hidden)\n",
        "\n",
        "            p = random.randrange(0, 100)\n",
        "\n",
        "        return buffer\n",
        "\n",
        "    def GA(self):\n",
        "\n",
        "        population = self.initPopulation()\n",
        "        buffer = population.copy()\n",
        "\n",
        "        best_solution = copy(population[0])\n",
        "        best_fitness = -sys.maxsize - 1\n",
        "\n",
        "        for i in range(Iterations):\n",
        "            population = self.calculateFittnes(population)\n",
        "\n",
        "            population = self.sortFitness(population)\n",
        "\n",
        "            if population[0].fitness > best_fitness:\n",
        "                best_solution = population[0]\n",
        "                best_fitness = population[0].fitness\n",
        "\n",
        "            print(CGREEN + \"Best Solution so far:\" + CEND)\n",
        "\n",
        "            print(best_fitness)\n",
        "\n",
        "\n",
        "            population = self.mate(population, buffer)\n",
        "\n",
        "        print()\n",
        "\n",
        "\n",
        "        print(CGREEN + \"Best Solution overall:\" + CEND)\n",
        "        print(CGREEN + \"Best Solution accuracy:\" + CEND)\n",
        "        print(best_fitness)\n",
        "\n",
        "        print(CGREEN + 'Best Solution depth: ' + CEND, best_solution.network.depth)\n",
        "        print(CGREEN + 'Best Solution layers: ' + CEND, best_solution.network.hidden)\n",
        "        print(CGREEN + 'Best Solution activation: ' + CEND, best_solution.network.activation)\n",
        "\n"
      ],
      "metadata": {
        "id": "y7TjA6Qk6ae-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Genetic:\")\n",
        "data_set = DS()\n",
        "K = InitPop()\n",
        "ga = K.GA()"
      ],
      "metadata": {
        "id": "1rf-QWfijg9v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10f27444-3676-4b58-dcfe-00f7d178745d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Genetic:\n",
            "\u001b[92mBest Solution so far:\u001b[0m\n",
            "1.0\n",
            "8\n",
            "5\n",
            "5\n",
            "8\n",
            "3\n",
            "4\n",
            "8\n",
            "6\n",
            "2\n",
            "8\n",
            "0\n",
            "6\n",
            "8\n",
            "5\n",
            "2\n",
            "8\n",
            "3\n",
            "0\n",
            "8\n",
            "0\n",
            "2\n",
            "8\n",
            "2\n",
            "5\n",
            "\u001b[92mBest Solution so far:\u001b[0m\n",
            "1.0\n",
            "8\n",
            "5\n",
            "1\n",
            "8\n",
            "3\n",
            "3\n",
            "8\n",
            "2\n",
            "5\n",
            "8\n",
            "4\n",
            "0\n",
            "8\n",
            "1\n",
            "0\n",
            "8\n",
            "0\n",
            "2\n",
            "8\n",
            "2\n",
            "4\n",
            "8\n",
            "2\n",
            "4\n",
            "\u001b[92mBest Solution so far:\u001b[0m\n",
            "1.0\n",
            "8\n",
            "0\n",
            "2\n",
            "8\n",
            "2\n",
            "1\n",
            "8\n",
            "0\n",
            "6\n",
            "8\n",
            "1\n",
            "4\n",
            "8\n",
            "1\n",
            "3\n",
            "8\n",
            "1\n",
            "6\n",
            "8\n",
            "6\n",
            "5\n",
            "8\n",
            "2\n",
            "2\n",
            "\u001b[92mBest Solution so far:\u001b[0m\n",
            "1.0\n",
            "8\n",
            "5\n",
            "2\n",
            "8\n",
            "1\n",
            "0\n",
            "8\n",
            "4\n",
            "4\n",
            "8\n",
            "0\n",
            "2\n",
            "8\n",
            "5\n",
            "0\n",
            "8\n",
            "0\n",
            "1\n",
            "8\n",
            "0\n",
            "6\n",
            "8\n",
            "5\n",
            "0\n",
            "\u001b[92mBest Solution so far:\u001b[0m\n",
            "1.0\n",
            "8\n",
            "3\n",
            "1\n",
            "8\n",
            "3\n",
            "6\n",
            "8\n",
            "0\n",
            "3\n",
            "8\n",
            "4\n",
            "1\n",
            "8\n",
            "0\n",
            "2\n",
            "8\n",
            "0\n",
            "3\n",
            "8\n",
            "5\n",
            "4\n",
            "8\n",
            "2\n",
            "4\n",
            "\u001b[92mBest Solution so far:\u001b[0m\n",
            "1.0\n",
            "8\n",
            "4\n",
            "2\n",
            "8\n",
            "2\n",
            "6\n",
            "8\n",
            "5\n",
            "5\n",
            "8\n",
            "3\n",
            "6\n",
            "8\n",
            "2\n",
            "4\n",
            "8\n",
            "2\n",
            "3\n",
            "8\n",
            "2\n",
            "3\n",
            "8\n",
            "1\n",
            "0\n",
            "\u001b[92mBest Solution so far:\u001b[0m\n",
            "1.0\n",
            "8\n",
            "4\n",
            "5\n",
            "8\n",
            "0\n",
            "1\n",
            "8\n",
            "4\n",
            "5\n",
            "8\n",
            "2\n",
            "5\n",
            "8\n",
            "3\n",
            "0\n",
            "8\n",
            "5\n",
            "6\n",
            "8\n",
            "3\n",
            "4\n",
            "8\n",
            "3\n",
            "1\n",
            "\u001b[92mBest Solution so far:\u001b[0m\n",
            "1.0\n",
            "8\n",
            "1\n",
            "4\n",
            "8\n",
            "1\n",
            "5\n",
            "8\n",
            "0\n",
            "4\n",
            "8\n",
            "6\n",
            "3\n",
            "8\n",
            "1\n",
            "1\n",
            "8\n",
            "0\n",
            "1\n",
            "8\n",
            "4\n",
            "3\n",
            "8\n",
            "6\n",
            "6\n",
            "\u001b[92mBest Solution so far:\u001b[0m\n",
            "1.0\n",
            "8\n",
            "1\n",
            "0\n",
            "8\n",
            "6\n",
            "1\n",
            "8\n",
            "4\n",
            "5\n",
            "8\n",
            "2\n",
            "3\n",
            "8\n",
            "5\n",
            "2\n",
            "8\n",
            "2\n",
            "0\n",
            "8\n",
            "0\n",
            "5\n",
            "8\n",
            "6\n",
            "2\n",
            "\n",
            "\u001b[92mBest Solution overall:\u001b[0m\n",
            "\u001b[92mBest Solution accuracy:\u001b[0m\n",
            "1.0\n",
            "\u001b[92mBest Solution depth: \u001b[0m 72\n",
            "\u001b[92mBest Solution layers: \u001b[0m [141, 197, 131, 183, 197, 131, 141, 197, 131, 183, 197, 131, 141, 197, 131, 141, 197, 131, 183, 197, 131, 183, 197, 141, 197, 197, 131, 141, 197, 131, 183, 197, 131, 183, 131, 183, 197, 141, 197, 197, 131, 141, 197, 131, 183, 197, 131, 183, 197, 183, 197, 131, 183, 197, 131, 141, 141, 141, 197, 141, 141, 141, 197, 131, 141, 197, 131, 183, 197, 131, 131, 119]\n",
            "\u001b[92mBest Solution activation: \u001b[0m tanh\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "AI_2022_Lab_5_B.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}